# -*- coding: utf-8 -*-
import nltk
# nltk.download('stopwords')

# calculating frequency of stop words
def stopwords_frequency(text):
    from nltk.corpus import stopwords
    english_stop_words = stopwords.words('english')
    stopword_count = 0
    for word in text.split():
        if word in english_stop_words:
            stopword_count = stopword_count + 1
    return stopword_count

# calculating the frequency ratio of stopwords vs text length
def stopwords_frequency_ratio(text):
    stopwords_count = stopwords_frequency(text)
    text_length = len(text.split())
    frequency_ratio = stopwords_count/float(text_length)
    return frequency_ratio

# calculating sentense length distribution
def sentence_length_distribution(text):
    # breakdown text into sentences
    sentences = nltk.tokenize.sent_tokenize(text.decode('utf-8').strip())
    # determine the distribution of sentences
        # simple sentence - has no conjunctions
        # compound sentence = has one conjunction joining two sentences
        # complex sentence = has one or more conjunctions
        # conjunctions can be either coordinating/subordinating
        # coordinating_conjunctions = ["for", "and", "nor", "but", "or", "yet", "so"]
        # subordinating_conjunctions = ["after", "as", "although", "because", "before", "even though", "if", "once", "rather than", "since", "that", "though", "unless", "until", "when", "whenever", "whereas", "while"]
    conjunctions = ["for", "and", "nor", "but", "or", "yet", "so","after", "as", "although", "because", "before", "even though", "if", "once", "rather than", "since", "that", "though", "unless", "until", "when", "whenever", "whereas", "while"]
    # number of conjunctions in sentences
    conjunction_distribution = []
    for sentence in sentences:
        sentence_conjunction_count = 0
        for word in sentence.split():
            if word in conjunctions:
                sentence_conjunction_count += 1
        conjunction_distribution.append(sentence_conjunction_count)

    simple_sentence_count = 0
    compound_sentence_count = 0
    complex_sentence_count = 0

    # categorizing sentences based on conjunction count
    for count in conjunction_distribution:
        if count == 0:
            simple_sentence_count += 1
        elif count == 1:
            compound_sentence_count += 1
        elif count > 1:
            complex_sentence_count += 1

    simple_sentence_distribution = simple_sentence_count/float(len(sentences))
    compound_sentence_distribution = compound_sentence_count/float(len(sentences))      
    complex_sentence_distribution = complex_sentence_count/float(len(sentences))      

    return [simple_sentence_distribution, compound_sentence_distribution, complex_sentence_distribution]


def load_dataset():
    # read file
    import csv
    labels = {'Faith Oneya':1, 'Bitange Ndemo':2, 'Abigail Arunga': 3}
    dataset = []
    with open('articles.csv') as csv_file:
        csv_reader = csv.reader(csv_file, delimiter=',')
        line_count = 0
        for row in csv_reader:
            if line_count == 0:
                line_count += 1
            else:
                dataset.append([labels[row[0]], row[1]])
                line_count += 1
    return dataset

# multinomial naive bayes
def multinomial_naive_bayes():
    dataset = load_dataset()
    # dataset size
    dataset_size = len(dataset)
    # count occurrences of each class
    class1_counter = 0
    class2_counter = 0
    class3_counter = 0
    for data_item in dataset:
        if data_item[0] == 1:
            class1_counter += 1
        elif data_item[0] == 2:
            class2_counter += 1 
        elif data_item[0] == 3:
            class3_counter += 1
    class1_occurrence_ratio = class1_counter/float(dataset_size)
    class2_occurrence_ratio = class2_counter/float(dataset_size) 
    class3_occurrence_ratio = class3_counter/float(dataset_size) 

    # get sentence length distribution and stop word frequency for all faith oneya's articles
    i = 0
    while i < 10:
        print(sentence_length_distribution(dataset[9+i][1]),stopwords_frequency_ratio(dataset[9+i][1]))
        i += 1




# print(sentence_length_distribution('When it’s not an integer, the highest probability number of events will be the nearest integer to the rate parameter, since the Poisson distribution is only defined for a discrete number of events. The discrete nature of the Poisson distribution is also why this is a probability mass function and not a density function. We can use the Poisson Distribution mass function to find the probability of observing a number of events over an interval generated by a Poisson process. Another use of the mass function equation — as we’ll see later — is to find the probability of waiting some time between events. For the problem we’ll solve with a Poisson distribution, we could continue with website failures, but I propose something grander. In my childhood, my father would often take me into our yard to observe (or try to observe) meteor showers. We were not space geeks, but watching objects from outer space burn up in the sky was enough to get us outside even though meteor showers always seemed to occur in the coldest months. The number of meteors seen can be modeled as a Poisson distribution because the meteors are independent, the average number of meteors per hour is constant (in the short term), and this is an approximation meteors don’t occur simultaneously. To characterize the Poisson distribution, all we need is the rate parameter which is the number of events/interval * interval length. From what I remember, we were told to expect 5 meteors per hour on average or 1 every 12 minutes. Due to the limited patience of a young child (especially on a freezing night), we never stayed out more than 60 minutes, so we’ll use that as the time period. Putting the two together, we get:'))
multinomial_naive_bayes()